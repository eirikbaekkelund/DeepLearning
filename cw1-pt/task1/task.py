import torch
import numpy as np
import time
import random
from torchvision.models import resnet50
from IPython import get_ipython

def polynomial_func(x, w=torch.tensor([1,2,3,4,5,0],dtype=torch.float32)):
    """
    Implements a polynomoial function that takes two input arguments, a weight vector w of size M + 1 
    and a scalar x. The function returns the value of the polynomial function at x. The polynomial function is vectorized for multiple pairs of scalar x and weight vector w.

    Args:
        w (torch array): Parameters of the polynomial function.
        x (scalar): Input data.

    Returns:
        np.ndarray: Output data.

    """

    x_powers = torch.pow(x.unsqueeze(1), torch.arange(len(w))) # [x^0, x^1, ..., x^M]
    x_powers = x_powers.reshape(x_powers.shape[0], -1) # Reshape x_powers to shape (n, M+1)
    return torch.matmul(x_powers, w.t()) # w[0]x^0 + w[1]x^2 + ... + w[M]x^M + w[M+1]x^(M+1)


def generate_data(sample_size, x_range=[-20,20], std_dev=0.2): 
    """ 
    Generates data in the range of x_range. The data is generated by adding Gaussian noise to the output of the polynomial function.
    The observed values are obtained by adding Gaussian noise to the output of the polynomial function.
    """
    x = torch.linspace(x_range[0], x_range[1], 100 + sample_size) # [x_1, x_2, ..., x_n]
    t = polynomial_func(x) # [y_1, y_2, ..., y_n]
    t += torch.normal(0, std_dev, t.shape) # [y_1 + noise_1, y_2 + noise_2, ..., y_n + noise_n]
    
    idx = list(range(sample_size + 100))
    random.shuffle(idx)

    t_train = t[idx[:sample_size]]
    t_test = t[idx[sample_size:]]
    x_train = x[idx[:sample_size]]
    x_test = x[idx[sample_size:]]

    return x_train.squeeze(), t_train.squeeze(), x_test.squeeze(), t_test.squeeze()

def fit_polynomial_ls(x, t, M):
    """ 
    Using linear least squares method to fit a polynomial function to the data.
    Takes M pairs of x and t and returns the optimal weight vector w.
    """
    powers = torch.arange(M+1).to(x.dtype) # [0, 1, ..., M]
    x_powers = torch.pow(x.unsqueeze(1), powers) # [x^0, x^1, ..., x^M]
    w = torch.matmul(torch.matmul(torch.inverse( torch.matmul(x_powers.t(), x_powers)) , x_powers.t()), t)
    
    return w


def fit_polynomial_sgd(x, t, M, lr, batch_size):
    """ 
    Runs a stochastic gradient descent for fitting polynomial functions with the 
    same arguments as fit_polynomial_ls in addition to the learning rate and the batch size.
    """
    w = torch.randn(M+1, requires_grad=True)
    optimizer = torch.optim.SGD([w], lr=lr)

    n_batches = len(x) // batch_size
    losses = []
    print_freq = 100
    print('LOSS: ')
    print('--------------------------------------------------------------------------------')
    
    for epoch in range(600):
        permutation = torch.randperm(len(x))
        for i in range(n_batches):
            indices = permutation[i*batch_size:(i+1)*batch_size]
            # t_batch = t[indices]
            t_batch = t.rename(None)[indices]
            x_batch = x[indices]
            optimizer.zero_grad()
            loss = torch.mean(torch.square((polynomial_func(x_batch, w) - t_batch)))
            loss.backward()
            losses.append(loss.item())
            optimizer.step()

        if epoch % print_freq == 0:
            print(f"Epoch: {epoch},\t | \t Loss: {loss.item()}")
        
    print('--------------------------------------------------------------------------------')
    return w



def print_report(M, lr=2.377e-10, batch_size=10, sample_size = [50,100], method='sgd'):
    """ 
    Print the mean and the standard deviation in difference between the observed training data
    and the underlying true polynomial function for the stochastic gradient descent/LS method.
    It also prints the difference between the predicted values and the underlying true polynomial.
    """
    assert method in 'ls' or 'sgd', "Method should be either \'ls\' or \'sgd\'"
    
    if method == 'sgd':
        print('Method: SGD')
        print("Hyperparameter Settings:\n")
        print("Learning rate:", lr)
        print("Batch size:", batch_size)
        print("--------------------------------------------------------------------------------\n")
    else:
        print('Method: LS')
        print("--------------------------------------------------------------------------------\n")
    
    for sample in sample_size:
        x_train, t_train, x_test, t_test = generate_data(sample)
        print("Sample size:", sample)
        print("--------------------------------------------------------------------------------")
        print(f"Target range (t): [{round(torch.min(t_test).numpy().tolist(),3)}, {round(torch.max(t_test).numpy().tolist(),3)}]")
        print(f"mean: {round(torch.mean(t_test).numpy().tolist(),3)}, std: {round(torch.std(t_test).numpy().tolist(),3)}")
        print('--------------------------------------------------------------------------------')

        if method == 'ls':
            w_pred = fit_polynomial_ls(x_train, t_train, M)
        else:
            w_pred = fit_polynomial_sgd(x_train, t_train, M, lr, batch_size)

        print("Polynomial degree: \t", M + 1)
        print('Actual weights: \t', [1,2,3,4,5])
        print("Predicted weights: \t", [round(elem, 4) for elem in w_pred.detach().numpy()])
        print("Mean Difference between true and predicted weights: ", round(torch.mean(torch.abs(w_pred - torch.tensor([1,2,3,4,5,0]))).detach().numpy().tolist(),4))
        print("Standard deviation of difference between true and predicted weights: ", round(torch.std(w_pred - torch.tensor([1,2,3,4,5,0])).detach().numpy().tolist(),4))
        print("--------------------------------------------------------------------------------")
        print("Mean Difference between true and predicted values: \t\t\t", round(torch.mean(torch.abs(polynomial_func(x_test, w_pred) - t_test)).detach().numpy().tolist(),4) )
        print("Standard deviation of difference between true and predicted values: \t", round(torch.std(polynomial_func(x_test, w_pred) - t_test).detach().numpy().tolist(),4))
        print("--------------------------------------------------------------------------------\n")

def compare_speed(M, sample_size = [50,100]):
    """ 
    Compare the speed of the least squares method and the stochastic gradient descent method.
    """
    print("Comparison of speed between the least squares method and the stochastic gradient descent method:\n")
    print("--------------------------------------------------------------------------------\n")
    for sample in sample_size:
        x_train, t_train, _, _ = generate_data(sample)
        print("Sample size:", sample)
        print("--------------------------------------------------------------------------------")

        start = time.time()
        fit_polynomial_ls(x_train, t_train, M)
        end = time.time()
        print("Least Squares Method: ", end - start, "seconds")

        start = time.time()
        fit_polynomial_sgd(x_train, t_train, M, lr=2.377e-10, batch_size=10)
        end = time.time()
        print("Stochastic Gradient Descent Method: ", end - start, "seconds")
        print("--------------------------------------------------------------------------------\n")

def learn_M(M_range, lrs, batch_size):
    """ 
    Returns the M that minimizes the loss for the stochastic gradient descent method.
    """
    losses = []
    best_loss = np.inf
    best_M = np.inf
    best_lr = np.inf

    for M, lr in zip(M_range,lrs):
        x_train, t_train, x_test, t_test = generate_data(train_size=100)
        print(f'Degree data polynomial = {M+1} \t|\t lr = {lr} \t|\t batch_size = {batch_size}')
        print('--------------------------------------------------------------------------------\n')
        w = fit_polynomial_sgd(x_train, t_train, M+1, lr, batch_size)
        loss = torch.mean(torch.square(polynomial_func(x_test, w) - t_test))
        losses.append(loss.item())
        if loss < best_loss:
            best_loss = loss
            best_M = M + 1
            best_lr = lr
        print('\n')
    return best_M, best_lr, best_loss

def print_learnead_M(M_range, lrs, batch_size):
    """ 
    Print the M that minimizes the loss for the stochastic gradient descent method.
    """
    print("M that minimizes the loss for the stochastic gradient descent method:\n")
    print("--------------------------------------------------------------------------------\n")
    print("Batch size:", batch_size)
    print("--------------------------------------------------------------------------------\n")
    print("M (degree):", learn_M(M_range, lrs, batch_size)[0])
    print("--------------------------------------------------------------------------------")

def report_M(M_range, lrs, batch_size, sample_size = [50,100]):
    """ 
    Print the mean and the standard deviation in difference between the observed training data
    and the underlying true polynomial function for the stochastic gradient descent method.
    It also prints the difference between the predicted sgd values and the underlying true polynomial.
    """
    print("--------------------------------------------------------------------------------\n")
    print("M that minimizes the loss for the stochastic gradient descent method:\n")
    print("--------------------------------------------------------------------------------\n")
    print("Batch size:", batch_size)
    for sample in sample_size:
        print('--------------------------------------------------------------------------------\n')
        print("Sample size:", sample)   
        print("--------------------------------------------------------------------------------\n")
        M, _, loss = learn_M(M_range, lrs, batch_size)
        print("--------------------------------------------------------------------------------")
        print(f"Best M (degree of polynomial):  {M}\t| Best Loss = {loss}\t| Sample size = {sample}")
        print("--------------------------------------------------------------------------------")
        # print("Learning rate:", lr)
        # print("Loss:", loss)
        print("\n")


if __name__ == '__main__':
    # print_report(M=5, method='ls')
    print('\n')
    print_report(M=5, lr=5e-13, batch_size=5, method='sgd')
    # print('\n')
    # compare_speed(M=5)
    # lr_schedule = [2e-2, 1e-3, 2e-5, 2e-8, 2e-10, 2e-18, 2e-22, 2e-24, 2e-26, 2e-28]
    # lr_schedule = [2e-2, 1e-3, 2e-5, 2e-5, 2e-5, 2e-5, 2e-5, 2e-5, 2e-5, 2e-8]
    # print('\n')
    # print_learnead_M(M_range=[i for i in range(1, 6)], lrs= [2e-2, 1e-3, 2e-5, 2e-8, 2e-10], batch_size = 10)
    # print('\n')
    # report_M(M_range=[i for i in range(1, 11)], lrs=[2e-2, 1e-3, 2e-5, 2e-8, 2e-10], batch_size = 10)